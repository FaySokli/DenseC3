# -*- coding: utf-8 -*-
"""Multi-Label.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h03zetwWB6EeP8Phg3tI1s2sl50AqtdF

Required packages:\
pandas==1.4.0\
numpy==1.21.5\
scikit-learn==1.0.2\
tensorflow==2.7.0\
torch==1.10.2\
transformers==4.17.0.dev0\
datasets==1.18.3\
textstat==0.7.2 (if running the ML part)\
xgboost==1.5.2 (if running the ML part)
"""

# pip install fsspec==2023.6.0
# pip install torch==2.2.1
# pip install accelerate==0.26.1
# pip install textstat
# pip install datasets
# python3 -m pip install pandas
# python3 -m pip install -U scikit-learn
# python3 -m pip install tensorflow
# python3 -m pip install transformers

import pandas as pd
import numpy as np

data = pd.read_csv("../Data/sample_full.csv")
data.fillna({'Remember': 0, 'Understand': 0, 'Apply': 0, 'Analyze': 0, 'Evaluate': 0, 'Create':0}, inplace=True)

LIWC_data = pd.read_csv("../Data/LIWC2015 Results (Learning_outcome.csv).csv")
data = data.join(LIWC_data).drop(['A'], axis=1)

data.head()

labels = data[data.columns[1:7]].values.tolist()

data.columns[1:7]


from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

"""## BERT"""

import torch
import tensorflow as tf
from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification, EarlyStoppingCallback
from datasets import load_metric
import logging
from tqdm import tqdm
import json

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EncodeDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

######################################################
######################################################

train_x, test_x, train_y, test_y = train_test_split(data['Learning_outcome'].tolist(), labels, test_size=0.2, random_state=666)
train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=666)

class BertMultiLabelClassifier:
    def __init__(self, model_name='bert-base-uncased', checkpoint_path='../Classifier/Checkpoints/checkpoint-490'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, problem_type="multi_label_classification")
        if checkpoint_path:
            self.model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path, num_labels=6, problem_type="multi_label_classification")
        else:
            self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6, problem_type="multi_label_classification")
        # self.model = AutoModelForSequenceClassification.from_pretrained('../Classifier/Checkpoints/checkpoint-490', num_labels=6, problem_type="multi_label_classification")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self.model.to(self.device)

    def train(self, train_x, train_y, val_x, val_y):
        train_encoded = self.tokenizer(train_x, truncation=True, padding=True, max_length=100)
        val_encoded   = self.tokenizer(val_x, truncation=True, padding=True, max_length=100)
        test_encoded  = self.tokenizer(test_x, truncation=True, padding=True, max_length=100)

        train_set, val_set, test_set = EncodeDataset(train_encoded, train_y), EncodeDataset(val_encoded, val_y), EncodeDataset(test_encoded, test_y)
        
        training_args = TrainingArguments(
                output_dir='../Classifier/Checkpoints',          # output directory
                overwrite_output_dir=True,
                num_train_epochs=3,              # total number of training epochs
                per_device_train_batch_size=64,  # batch size per device during training
                per_device_eval_batch_size=64,   # batch size for evaluation
                warmup_steps=5,                  # number of warmup steps for learning rate scheduler
                weight_decay=0.05,               # strength of weight decay
                logging_dir='./logs',            # directory for storing logs
                logging_steps=10,
                evaluation_strategy="steps",
                save_strategy="steps",
                save_steps=10,
                load_best_model_at_end=True
            )
        
        trainer = Trainer(model=self.model, args=training_args, train_dataset=train_set, eval_dataset=val_set, callbacks=[EarlyStoppingCallback(early_stopping_patience=5)])
        trainer.train()
        logits = trainer.predict(test_set)
        logits.predictions.shape
        predicted = tf.keras.activations.sigmoid(logits.predictions)
        predicted.numpy()
        predicted_label = self.getClassResult(predicted)
        return predicted_label, logits

    def getClassResult(self, predicted):
        results = []
        for probs in predicted.numpy():
            result = []
            for prob in probs:
                if prob < 0.5:
                    result.append(0)
                else:
                    result.append(1)
            results.append(result)
        return results

    def compute_metrics(self, eval_pred):
        metric = load_metric("f1")
        logits, labels = eval_pred
        predictions = tf.keras.activations.sigmoid(logits)
        predicted = self.getClassResult(predictions)
        return metric.compute(predictions=predicted, references=labels, average="micro")

    def predict_labels(self, text_ids, texts, batch_size=16):
        preds = []
        for i in tqdm(range(0, len(texts), batch_size), desc="Predicting labels..."):
            batch_text_ids = text_ids[i:i+batch_size].tolist()
            batch_texts = texts[i:i+batch_size].tolist()
            inputs = self.tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)
            inputs = {name: tensor.to(self.device) for name, tensor in inputs.items()}
            with torch.no_grad():
                outputs = self.model(**inputs)
            _, batch_preds_indices = torch.max(outputs.logits, dim=1)
            for j in range(len(batch_text_ids)):
                preds.append({"_id": batch_text_ids[j], "category": str(batch_preds_indices[j].item())})
        return preds

    def predict_logits(self, text_ids, texts, batch_size=16):
        text_id_to_logits = {}
        for i in tqdm(range(0, len(texts), batch_size), desc="Predicting logits..."):
            batch_texts = texts[i:i+batch_size]
            inputs = self.tokenizer(batch_texts.to_list(), return_tensors='pt', padding=True, truncation=True, max_length=512)
            inputs = {name: tensor.to(self.device) for name, tensor in inputs.items()}
            with torch.no_grad():
                outputs = self.model(**inputs).logits
            for j in range(outputs.shape[0]):
                text_id_to_logits[text_ids.iloc[i+j]] = outputs[j].cpu().tolist()
            
        return text_id_to_logits
######################################################
######################################################
train = False
if train:
    classifier = BertMultiLabelClassifier('bert-base-uncased', None)
    predicted_label, logits = classifier.train(train_x, train_y, val_x, val_y)
    count = 0
    for pred in predicted_label:
        if pred.count(1) > 1:
            count += 1
    count
    print(classification_report(test_y, predicted_label, output_dict=False, target_names=list(data.columns[1:7]), digits=3))

    # roc_auc_score(test_y, predicted_label.numpy(), average=None)

    accuracy_score(np.array(test_y), predicted_label)

    dl_result_df = pd.DataFrame(data=predicted_label, columns=data.columns[1:7])

else:
    classifier = BertMultiLabelClassifier('bert-base-uncased', '../Classifier/Checkpoints/checkpoint-490')


######################################################
# Computer Science
######################################################
corpus_path = '../multi-domain/computer_science/collection.jsonl'
corpusCS = pd.read_json(corpus_path, lines=True)
CS_logits = classifier.predict_logits(corpusCS['_id'], corpusCS['text'])
CS_labels = classifier.predict_labels(corpusCS['_id'], corpusCS['text'])
CS_logits_str = {str(key): value for key, value in CS_logits.items()}
with open('../multi-domain/computer_science/blooms/CS_logits.json', 'w') as json_file:
    json.dump(CS_logits_str, json_file)
with open('../multi-domain/computer_science/blooms/CS_labels.jsonl', 'w') as jsonl_file:
    for record in CS_labels:
        jsonl_file.write(json.dumps(record) + '\n')

######################################################
# Political Science
######################################################
corpus_path = '../multi-domain/political_science/collection.jsonl'
corpusPS = pd.read_json(corpus_path, lines=True)
PS_logits = classifier.predict_logits(corpusPS['_id'], corpusPS['text'])
PS_labels = classifier.predict_labels(corpusPS['_id'], corpusPS['text'])
PS_logits_str = {str(key): value for key, value in PS_logits.items()}
with open('../multi-domain/political_science/blooms/PS_logits.json', 'w') as json_file:
    json.dump(PS_logits_str, json_file)
with open('../multi-domain/political_science/blooms/PS_labels.jsonl', 'w') as jsonl_file:
    for record in PS_labels:
        jsonl_file.write(json.dumps(record) + '\n')

######################################################
# HotpotQA
######################################################
corpus_path = '../hotpotqa/corpus.jsonl'
corpusH = pd.read_json(corpus_path, lines=True)
hotpotqa_logits = classifier.predict_logits(corpusH['_id'], corpusH['text'])
hotpotqa_labels = classifier.predict_labels(corpusH['_id'], corpusH['text'])
hotpotqa_logits_str = {str(key): value for key, value in hotpotqa_logits.items()}
with open('../hotpotqa/blooms/hotpotqa_logits.json', 'w') as json_file:
    json.dump(hotpotqa_logits_str, json_file)
with open('../hotpotqa/blooms/hotpotqa_labels.jsonl', 'w') as jsonl_file:
    for record in hotpotqa_labels:
        jsonl_file.write(json.dumps(record) + '\n')

######################################################
# NATURAL QUESTIONS
######################################################
corpus_path = '../nq/corpus.jsonl'
corpusNQ = pd.read_json(corpus_path, lines=True)
nq_logits = classifier.predict_logits(corpusNQ['_id'], corpusNQ['text'])
nq_labels = classifier.predict_labels(corpusNQ['_id'], corpusNQ['text'])
with open('../nq/blooms/nq_logits.json', 'w') as json_file:
    json.dump(nq_logits, json_file)
with open('../nq/blooms/nq_labels.jsonl', 'w') as jsonl_file:
    for record in nq_labels:
        jsonl_file.write(json.dumps(record) + '\n')

corpus_path = '../nq-train/corpus.jsonl'
corpusNQtrain = pd.read_json(corpus_path, lines=True)
nqtrain_logits = classifier.predict_logits(corpusNQtrain['_id'], corpusNQtrain['text'])
nqtrain_labels = classifier.predict_labels(corpusNQtrain['_id'], corpusNQtrain['text'])
with open('../nq-train/blooms/nqtrain_logits.json', 'w') as json_file:
    json.dump(nqtrain_logits, json_file)
with open('../nq-train/blooms/nqtrain_labels.jsonl', 'w') as jsonl_file:
    for record in nqtrain_labels:
        jsonl_file.write(json.dumps(record) + '\n')
